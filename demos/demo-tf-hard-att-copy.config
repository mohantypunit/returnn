#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

# Also see demo-tf-att-copy.config for a soft attention variant of the same task.

# network topology based on:
# https://github.com/rwth-i6/returnn-experiments/blob/master/2019-librispeech-system/attention/base2.bs18k.curric3.config
# adapted/simplified/extended for hard attention

# This is still work-in-progress. It does not quite work yet... Also has some cheating in it...

import os
from Util import get_login_username
demo_name, _ = os.path.splitext(__file__)
print("Hello, experiment: %s" % demo_name)

# task
use_tensorflow = True
task = config.value("task", "train")

if task == "train":
    beam_size = 1
else:
    beam_size = 12

# data
num_inputs = 10
num_outputs = {"data": [num_inputs,1], "classes": [num_inputs,1]}
train = {"class": "CopyTaskDataset", "nsymbols": num_inputs, "num_seqs": 1000, "minlen": 5, "maxlen": 5}
#dev = {"class": "CopyTaskDataset", "nsymbols": num_inputs, "num_seqs": 50, "minlen": 1, "maxlen": 20, "fixed_random_seed": 42}
dev = {"class": "CopyTaskDataset", "nsymbols": num_inputs, "num_seqs": 50, "minlen": 20, "maxlen": 20, "fixed_random_seed": 42}

batch_size = 5000
max_seqs = 10


EncKeyTotalDim = 20
EncValueTotalDim = 20
target = "classes"


def rel_embed(source, network):
    import tensorflow as tf
    from TFUtil import nd_indices, expand_dims_unbroadcast, where_bc
    x = source(0, as_data=True, auto_convert=False)  # (B, T, K)
    v = source(1, as_data=True, auto_convert=False)  # (B, Ts, K)
    assert v.dim == x.dim
    t = source(2, auto_convert=False)  # (B,)
    vi = source(3, as_data=True, auto_convert=False)  # (K,)
    assert vi.batch_shape == (x.dim,)
    #t = tf.Print(t, ["t:", t])
    time_dim = tf.shape(x.placeholder)[x.time_dim_axis]
    batch_dim = tf.shape(x.placeholder)[x.batch_dim_axis]
    assert len(v.shape) == 2 and all([isinstance(d, int) for d in v.shape])
    ts_dim = v.shape[0]
    indices = tf.expand_dims(tf.range(ts_dim), axis=0)  # (1,Ts)
    indices = indices + tf.expand_dims(t, axis=1)  # (B,Ts)
    indices = tf.minimum(indices, time_dim - 1)
    indices = nd_indices(indices)  # (B,Ts,2)
    x0 = tf.scatter_nd(indices=indices, updates=v.placeholder, shape=[batch_dim, time_dim, x.dim])  # (B,T,K)
    #if x.time_dim_axis == 1:
    #    x0 = tf.transpose(x0, [1, 0, 2])
    i = network.get_rec_step_index()
    x1 = tf.scatter_nd([[i]], [vi.placeholder], [time_dim, x.dim])  # (T,K). cheating
    x1 = expand_dims_unbroadcast(x1, axis=0, dim=batch_dim)  # (B,T,K)
    #x1 = tf.Print(x1, ["i:", i, "t:", t], summarize=5)
    #return x.placeholder + x0 + x1
    #return x0
    return x1


def energy_pp(source, network):
    import tensorflow as tf
    from TFUtil import nd_indices, expand_dims_unbroadcast, where_bc
    #return source(0)
    x = source(0, as_data=True)
    batch_dim = tf.shape(x.placeholder)[x.batch_dim_axis]
    time_dim = tf.shape(x.placeholder)[x.time_dim_axis]
    i = network.get_rec_step_index()
    #x1 = tf.scatter_nd([[i]], [1.0], [time_dim])  # (T)
    # Cheating:
    x1 = where_bc(tf.equal(tf.range(time_dim), i), 0.0, float("-inf"))
    x1 = expand_dims_unbroadcast(x1, axis=0, dim=batch_dim)  # (B,T)
    assert x.batch_dim_axis == 0 and x.time_dim_axis == 1
    #x1 = tf.Print(x1, ["i:", i, "energy:", x1[0]], summarize=10)
    return x1


def loss_ce(source):
    import tensorflow as tf
    from TFUtil import nd_indices, safe_log
    x = source(0, auto_convert=False, as_data=True).copy_as_batch_major()
    y = source(1, auto_convert=False, as_data=True).copy_as_batch_major()
    assert y.batch_ndim == 1 and x.batch_ndim == y.batch_ndim + 1 and x.dim == y.dim
    x_ = safe_log(x.placeholder)
    assert x_.op.type != "Log"  # it means we used LogSoftmax or so
    out = -tf.gather_nd(x_, nd_indices(y.placeholder))
    #out = tf.Print(out, [x.name, "loss", out, "shape", tf.shape(out)])
    return out
    #return tf.zeros_like(out)


network = {
"input": {"class": "linear", "activation": "tanh", "n_out": 20},

"encoder": {"class": "copy", "from": "input"},  # dim: EncValueTotalDim
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": "encoder", "n_out": EncKeyTotalDim},
"enc_value": {"class": "copy", "from": "encoder"},  # (B, enc-T, D)

"output": {"class": "rec", "from": [], 'only_on_search': True, 'cheating': config.bool("cheating", False), "unit": {
    "s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": "s", "n_out": EncKeyTotalDim},
    "t_rel_var": {"class": "variable", "shape": (5, EncKeyTotalDim)},
    "i_var": {"class": "variable", "shape": (EncKeyTotalDim,), "add_batch_axis": False},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "s_transformed"], "n_out": EncKeyTotalDim},
    "energy_in1": {"class": "eval", "from": ["energy_in", "t_rel_var", "prev:t", "i_var"],
        "eval": "self.network.get_config().typed_value('rel_embed')(source, network=self.network)"},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": "energy_in1"},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": "energy_tanh", "n_out": 1},  # (B, enc-T, 1)
    "energy0": {"class": "squeeze", "axis": "f", "from": "energy"},  # (B, enc-T)
    "energy1": {"class": "eval", "from": "energy0",
        "eval": "self.network.get_config().typed_value('energy_pp')(source, network=self.network)"},
    "att_weights": {"class": "softmax_over_spatial", "from": "energy1", "use_time_mask": False},  # (B, enc-T)
    #"att_weights": {"class": "softmax_over_spatial", "from": "energy1", "start": "prev:t"},  # (B, enc-T)
    # ChoiceLayer works on the feature axis.
    "att_weights0": {"class": "reinterpret_data", "from": "att_weights", "set_axes": {"f": "t"}},

    "t": {
        "class": "choice", "from": "att_weights0", "target": None, "beam_size": beam_size,
        "base_beam_score_scale": 0.0 if task == "train" else 1.0,  # later remove...
        "length_normalization": False, "initial_output": 0},  # (B,)
    #"t": {"class": "print", "from": "t0", "initial_output": 0},
    # collocate_with to have it with the current beam
    #"t_ce": {
    #    "class": "eval", "from": ["att_weights0", "t"], "eval": "self.network.get_config().typed_value('loss_ce')(source)",
    #    "loss": "as_is", "collocate_with": "t",
    #    "out_type": {"shape": (), "feature_dim_axis": None, "time_dim_axis": None, "dtype": "float32"}},

    "att0": {"class": "gather_nd", "position": "t", "from": "base:enc_value"},  # (B, V)
    "att1": {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"},  # (B, V)
    #"att": {"class": "eval", "from": ["att0", "att1"], "eval": "source(0) * 0.5 + source(1) * 0.5"},
    "att": {"class": "eval", "from": ["att0", "att1"], "eval": "source(0) * 1. + source(1) * 0."},

    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:target_embed", "prev:att"], "n_out": 20},
    "readout_in": {"class": "linear", "from": ["s", "prev:target_embed", "att"], "activation": None, "n_out": 50},
    "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": "readout_in"},
    "output_prob": {"class": "softmax", "from": "readout", "target": target},

    'output': {
        'class': 'choice', 'target': target, 'search': task != 'train',
        'prob_scale': 0.0 if task == "train" else 1.0,  # remove this later if we fix training...
        'beam_size': beam_size, 'cheating': config.bool("cheating", False), 'from': "output_prob",
        "initial_output": 0},
    "output_ce": {
        "class": "eval", "from": ["output_prob", "output"], "eval": "self.network.get_config().typed_value('loss_ce')(source)",
        "loss": "as_is", "collocate_with": "output",
        "out_type": {"shape": (), "feature_dim_axis": None, "time_dim_axis": None, "dtype": "float32"}},

    "end": {"class": "compare", "from": "output", "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': 'output', "n_out": 20},

}, "target": target, "max_seq_len": "max_len_from('base:encoder')"},

#"decide": {"class": "decide", "from": "output", "loss": "search_score", "only_on_search": True}

}

search_train_network_layers = ["output"]
if "decide" in network:
    search_train_network_layers.append("decide")
debug_print_layer_output_template = True

stop_on_nonfinite_train_score = False

adam = True
learning_rate = 0.001
model = "/tmp/%s/crnn/%s/model" % (get_login_username(), demo_name)
num_epochs = 100
log_verbosity = 4  # 5
